# @package optim

base:
  _target_: torch.optim.Adam
  lr: 1e-4
  weight_decay: 4e-4
head:
  _target_: torch.optim.Adam
  lr: 1e-3
  weight_decay: 4e-4
base_lr_scheduler:
  _target_:  torch.optim.lr_scheduler.MultiStepLR
  milestones: [30, 60, 120]
  gamma: 0.5
head_lr_scheduler:
  _target_:  torch.optim.lr_scheduler.MultiStepLR
  milestones: [20, 40, 60, 120]
  gamma: 0.5